---
format:
  chribel-summary-quarto-pdf:
    include-in-header:
    - text: "\\usepackage[datesep=.]{datetime2}"
    - text: "\\DTMsetdatestyle{ddmmyyyy}"
    - text: "\\usepackage{blindtext}"
    toc: true
    classoption: twocolumn

# [DOCUMENT INFORMATION]
title: "Embedded Systems and Advanced Computing"
subtitle: "ENCE464"
author: "Andy Ming"

# [PAGE OPTIONS]
lang: en-GB
babel-lang: ukenglish

# [HEADER & FOOTER]
fancyhdr:
  header:
    right: "Embedded Systems & Advanced Computing"
    center: ""
    left: "University of Canterbury"
  footer:
    right: "ENCE464"
    center: "\\thepage\\ / \\pageref{LastPage}"
    left: "\\today"
  
source:
  github: "https://www.youtube.com/watch?v=VGhcSupkNs8"

accentcolor: "124E82" # must be given as hex, sadly :(

chribel-fontfamily:
  - name: AlegreyaSans      # used for section headings, title page
  - name: cmbright          # used for paragraph and math
  - name: inconsolata
    options: "scaled=0.95"  # for code blocks
---

# How to work code

Remember that software engineering is 50-70% maintenance. Because modern machines heavily rely on microcontrollers there is great demand.sol

![](imagesi/paste-3.png){fig-align="center" width="30%"}

Software engineering has many different aspects (the dark blue ones are focused on here), find out more [here](https://www.computer.org/education/bodies-of-knowledge/software-engineering/topics).

![](imagesi/paste-1.png){fig-align="center" width="25%"}

## Feature Branches

To implement different features, use a branch per feature, this guarantees that the main is always in working condition.

![](imagesi/paste-2.png)

::: callout-important
## Branching Rules

-   Feature branches are **temporary** branches for new features, improvements, bug fixes or refactorings.
-   Don't push directly to **master/main**.
-   Each feature branch is owned by **one** developer.
-   Only do merge requests on **complete** changes i.e. [don’t break main]{.underline}.
-   Thoroughly test your change prior to **starting** AND prior to **completing** a merge request.
-   Use your commit messages to tell the **story** of your development process.
:::

To minimise integration issues:

-   A feature branch should only hold a small increment of change
-   If main is updated during feature development, merge the new main into your feature branch **locally**, **before** making a merge request

## Clean Code

::: callout-warning
## Smells of Bad Code

-   *Rigidity*: Changing a single behaviour requires changes in many places
-   *Fragility*: Changing a single behaviour causes malfunctions in unconnected parts
-   *Inseparability*: Code can't be reused elsewhere
-   *Unreadability*: Original intent can't be derived from code
:::

### Reveal Intent

``` cpp
// BAD
uint16_t adcAv;  // Average Altitude ADC counts
// GOOD
uint16_t averageAltitudeAdc;
```

### Don't Repeat Yourself (DRY)

Avoid duplicate code $\rightarrow$ Put it into a function. Can you put it in a function? Then you should!

### Consistent Abstraction

**High-Level** ideas shouldn't get lost in **Low-Level** operations.

``` cpp
// Bad Example
deviceState.newGoal = readADC() * POT_SCALE_COEFF;   // low-level
deviceState.newGoal = (deviceState.newGoal / STEP_GOAL_ROUNDING)*STEP_GOAL_ROUNDING;  // high-level
```

### Encapsulation

-   *Hide* as much as possible
-   *Public* Interface: Header File, only declare what other modules need to know
-   *Private* / Inner Workings: Source File
-   *Avoid* global variables $\rightarrow$ Use *getter* & *setter*

### Comments

More comments $\neq$ better quality. Use comments only to:

1.  Reveal intent after you tried everything else
2.  Document public APIs - sometimes

### Code Reviews

Use *merge requests*, label feeback as *bug, code, quality, preference*.

![](imagesi/paste-14.png){fig-align="center" width="40%"}

## SOLID

How to make designs **flexible**, as requirements change all the time (Agile).

![](imagesi/paste-31.png)

*SOLID* is all about **managing dependencies**.

![](imagesi/paste-32.png)

### Single Responsibility Principle (SRP)

*"A module should only have a single responsibility. It should only have one reason to change"*

Instead of `ADC_read` which handles the ADC reading, averaging and the setting of the new goal, we breack it up into a module `ADC_HAL` which handles the ADC reading and averaging and the module `new_goal_reader` which just handles the setting of the new goal.

-   A module does **one thing** and does it **well**
-   Use good names, reveal intent
-   Don't access numerous data structures and globals
-   If the high level requirement changes, it only affects a single module
-   "Hide design decisions"

### Open-Closed Principle (OCP)

*"Software entities (modules, functions) should be **open to extension, but closed for modification**"*

The `new_goal_reader` doesn't have to be modified in case of a hardware change. But it's functionality can be extended through swapping out the HAL implementation.

-   Changes through adding code instead of modifying
-   aka USB standard: Plug-n-Play, no hardware change needed
-   Abstraction of the connection
-   OOP use "interface" or "abstract class"
-   C use "header files" or "function pointers"

### Liskov Substitution Principle (LSP)

*"Subtypes should be substitutable with their base types"*

`TIVA ADC HAL` is perfectly substitutable with its base type of the `ADC HAL` interface.

-   Exchanged modules behave the same way, no matter of the subtype
-   Under the hood might do stuff differently but leads to the same output

### Interface Segregation Principle (ISP)

*"Clients should not be forced to depend on functions that they do not use"*

The interface `ADC HAL` is defined on the needs of `new_goal_reader` (the client). Don't show unneeded functions.

-   Write "small" interfaces
-   Allows to limit dependencies
-   Seperating concerns

### Dependecy inversion Principle (DIP)

*"High-level modules should not depend upon low-level modules. Both should depend on abstraction"*

The interface `ADC HAL` is the abstraction and both, `new_goal_reader` and `TIVA ADC HAL`, are implementing this. `new_goal_reader` doesn't know (and care) which implementation is called.

In OOP this is called **Polymorphism**.

## SOLID Models for C

::: callout-tip
## How much design is enough?

-   Use simplest flexible design for today's requirements
-   Changing requirements $\rightarrow$ Change **Design**
-   Tests ensure functionality is retained
:::

### Single Instance Model

![](imagesi/paste-33.png){fig-align="center" width="7cm"}

### Multiple Instance Model

-   Create multiple instances
-   Use abstract data type (ADT) $\rightarrow$ object definitions and details are **encapsulated**
-   Public functions to operate on the abstract data type `f(O,x)`
-   **Multiple** instance, **Static** binding

``` cpp
// In header file
typedef struc circBuf circBuf_t;

// In source file
struct circBuf {
    uint32_t size;
    uint32_t windex;
    uint32_t rindex;
    int32_t *data;
}
```

### Dynamic Interface

![](imagesi/paste-34.png){fig-align="center" width="7cm"}

-   Configuration determined in runtime
-   Interface is a *public struct of function pointers*
-   **Single** instance, **Dynamic** binding

### Pre-Type Dynamic Interface

-   Support any number & combination of drivers
-   Each type has its own constructor
-   Objects cast to abstract interface
-   Have a array of abstract instances
-   **Multiple** instance, **Dynamic** binding

## Design Patterns

23 patterns introduced by the **G**ang **o**f **F**our (GoF), for **Dependency Management**. There are 3 types:

-   **Creational**: Create instances of objects
-   **Structural**: Set communication pathways
-   **Behavioural**: Distribute responsibilities

![](imagesi/paste-35.png){fig-align="center" width="8cm"}

::: callout-tip
## Recommendations

-   Don't overcomplicate, design for todays requirements
-   Use pattern if beneficial, maybe simple code is sufficient
-   Customise patterns to application
:::

### Adapter Pattern

-   What: Wrap adapter around another module to give it a more desirable interface
-   Hiding ugly interfaces of a 3rd-party service
-   Hiding data conversions
-   Make incompatible modules compatible
-   Fulfills SRP, ISP, LSP

![](imagesi/paste-36.png){fig-align="center" width="7cm" height="2.4cm"}

### State Pattern

-   What: Module will behave differently depending on internal states
-   Allows implementation of FSM without several lengthy conditional statements
-   State is saved in the private `currentState` variable
-   There is a module implementation per state
-   The client initialises the context with a state
-   Fulfills SRP, OCP

![](imagesi/paste-37.png){fig-align="center" width="9cm" height="2.4cm"}

### Command Pattern

-   What: Turns request into a stand-alone object
-   Invoker calls receiver through command object
-   Client attaches invoker with its commands
-   Multiple things can execute one command
-   Command is placed in queue until receiver is ready
-   Triggers can invoke series of commands
-   Fulfills SRP, OCP

![](imagesi/paste-38.png){fig-align="center" width="8.5cm"}

## Legacy / Vererbt / Veralteter Code

If handed bad quality (no tests, no encapsulation, no good practices, ...) and you have to add features you can either (1) *add to the mess by hacking in new features* or (2) *rewrite code from scratch*.

The issues are (1) will *reduce productivity* and it's *easy to introduce bugs* but (2) is very *time consuming*, it's *difficult to maintain two versions* (there might still be old versions in the field which have to be maintained) and there will be a *new set of bugs* to deal with.

So we try to refactor until it's easy to make changes. To preserve functionality we iterate *in small increments* with **Trageted Tests** *(allows changes and new features to happen at the same time)*:

1.  **Write tests** of the code you need to change
2.  **Test drive** changes to legacy code
3.  **Test drive** new code
4.  **Refactor** tested area

Also add **Characterisation Tests** where understanding is required. *Tests are a living documentation*.

To make sure *key functionality* isn't altered, add **Strategic Tests**. (e.g. control algorithm, safety-critical error detection, ...)

::: callout-note
## Putting Code Under Test

1.  Identify are of code to test (targeted, characterisation, strategic)
2.  Find test points (function calls, global variables $\rightarrow$ encapsulate ASAP, serial)
3.  Break dependencies (Solitary tests)

![](imagesi/paste-21.png){fig-align="center" width="8cm" height="4.6cm"}
:::

## Designing with Models

### Event Driven State Machine

![](imagesi/paste-55.png){fig-align="center" width="7cm"}

With a model there are already some flaws which arise. For example: What happens if the door is closing and a valid card is wiped?

Programm Execution:

-   State: PC + Variable Values
-   Transitions: Instructions

### Time Driven State Machine

PID-Controllers are state machines with "near infinite" states

![](imagesi/paste-56.png){fig-align="center" width="7cm"}

### Use modelling languages

The following example is written in [PLUSCAL](https://lamport.azurewebsites.net/tla/tutorial/intro.html)

``` cpp
Light == {UNLIT, RED, GREEN, AMBER} 
Direction == {NS, EW}
    
variables
    state = [dir \in Direction |-> UNLIT];

process Lights \in Direction 
begin
    Cycle:
        either await state[self] = RED;
            state[self] := GREEN;
        or await state[self] = GREEN;
            state[self] := AMBER;
        or await state[self] = AMBER;
            state[self] := RED;
        end either; 
    goto Cycle; 
end process;

// Define a Requirement
define 
    Safe =/ ~(state[NS] = GREEN / state[EW] = GREEN) 
end define;
```

The model can be checked and we can verify if the requirement is met or not. Add to the model until it meets all requirements

![](imagesi/paste-57.png){fig-align="center" width="8cm"}

# Embedded Software Design

## Architecture

Architecture are "important" structures, every structure is important for a specific part of the software. There are several different structures in embedded software systems.

::: callout-note
## Architecture Goals

-   *Understandability* - In Development & Maintenance
-   *Modifiability* - Through "best practices"
-   *Performance* - Reduce Overheads

Other possible requirements: Portability, Testability, Maintainability, Scalability, Robustness, Availability, Safety, Security
:::

**Static Structures:** Conceptual abstraction a developer works with

![](imagesi/paste-4.png){fig-align="center" width="40%"}

**Dynamic Structures:** Relationships that exist in executing software

![](imagesi/paste-9.png){fig-align="center" width="40%"}

**Allocation Structures:** Assignment of software elements to external things

![](imagesi/paste-10.png){fig-align="center" width="40%"}

Patterns are always a combination of tactics, depending on what you're trying to achieve.

![](imagesi/paste-11.png)

::: callout-important
## Trade-Offs

Quality attributes can conflict with each other. For example:

![](imagesi/paste-12.png){fig-align="center" width="60%"}
:::

::: callout-note
## Keep Record of Decisions

To keep record of decisions and to not loose the overview use tools like:

-   *Architecture Haikus*: A onepager overview of your document [see here or in the appendix folder](https://ieeexplore.ieee.org/document/7093016).

-   *Architecture Decision Records*: A incremental document to record decisions on the go [either in a tool or a markdown file](https://adr.github.io/).
:::

### Layered

Each layer is is providing services to the above layer through well-defined interfaces. Each layer can only interact with the layer directly above or below.

Supports portability and modifiability by allowing internal changes to be made inside a layer without impacting other layers, and isolating changes in layer-to-layer interfaces from more distant layers.

![](imagesi/paste-5.png){fig-align="center" width="30%"}

### Ports-and-Adapters (or *Hexagonal*)

Introduces a single core logic which communicates through abstraction interfaces (**Ports**) to different modules. The **Adapters** map the external interactions to the standard interface of the port.

Supports portability and testability by making the inputs to the ports independent of any specific source, and supports modifiability by creating a loose coupling between components.

![](imagesi/paste-6.png){fig-align="center" width="40%"}

### Pipes-and-Filters

Supports modifiability through loose coupling between components, and performance by introducing opportunities for parallel execution.

![](imagesi/paste-7.png){fig-align="center" width="40%"}

### Microkernel

RTOS is a implementation of a Microkernel Architecture. The **Microkernel** includes a set of common core services. Specific services (**Tasks**) can be plugged into the kernel.

Supports modifiability and portability.

![](imagesi/paste-8.png){fig-align="center" width="25%"}

#### Tasks for Priority and Modularity

\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\begin{itemize}
    \item[+] Control Priority
    \item[+] Control Response Time
    \item[+] Modularity
    \item[-] Concurrency Issues
    \item[-] Overhead (Scheduler)
    \item[-] Starvation (Task gets no CPU time)
\end{itemize}

![](imagesi/paste-27.png){fig-align="center" width="6cm"}

Use **just enough** tasks.

## RTOS

To improve **Performance** we introduce **Concurrency** (Run tasks in parallel).

![](imagesi/paste-15.png){fig-align="center" width="25%"}

Preemptive approach: *Separation* of concerns, *Scalability*, State is *Managed.*

Do the ***right thing*** at the ***right time*** $W$

![](imagesi/paste-17.png){fig-align="center" width="40%"}

::: callout-note
## RTOS vs. Desktop OS

-   Desktop OSs don't try to achieve *hard* real-time performance
-   In a Desktop OS, programs can be loaded in runtime
-   RTOS is compiled as part of the application, to add a new "program" the application has to be recompiled
:::

### Tasks

``` cpp
xTaskCreate(
    BlinkTask,    // Function that defines task 
    "Blink Task", // Task name (used in debugging) 
    STACK_SIZE,   // No. of 4-byte words for stack
    NULL,         // Optional pointer to task argument 
    PRIORITY,     // Higher number = higher priority 
    NULL);        // Optional pointer to task handle
```

Taskswitches happen at *scheduling points* which occur when a **task is blocked**, **interrupt causes a task**, **priority change**, **higher priority task gets ready** or **system tick interrupt**.

![](imagesi/task_states.png){fig-align="center" width="40%"}

#### Stack Size

::: callout-important
## Stack Size Value

The stack size value passed in `xTaskCreate` is measured in **4-Byte** words.

Set high margins, something like **300%**
:::

-   Minimal: `configMINIMAL_STACK_SIZE`

-   Maximal: Device *RAM*

-   Actually: Analyse

    -   Dynamic: Set something and see if it works / use `uxTaskGetStackHighWaterMark` to measure

    -   Static: Use tools (e.g. GCC `-fstack-usage`) to attempt reading on how much stack is needed per function

#### Priority

Task priority has a strong influence on when a task is run and thus on the overall behaviours of the application.

**Assign priority based on importance**

1.  Separate tasks into "critical" (hard deadline) and "non-critical" (soft deadline)
2.  Assign low priority to non-critical tasks
3.  To be sure about critical tasks meeting their deadlines, apply scheduling theory

**Assign non-critical tasks to low priorities**

1.  Either apply the same priority for all non-critical tasks
2.  Or prioritise by *importance*, which depends on
    a.  Shortness of Deadline
    b.  Frequency of Execution
    c.  Need for Precessor time

**Assign critical tasks *deadline monotonic* priorities**

Apply priority based on the size of it's deadline.

1.  Highest $\leftarrow$ shortest deadline
2.  Lowest $\leftarrow$ longest deadline

::: callout-tip
## Deadline / Rate Monotonic Priorities

Deadlines $D_i$ and Period $T_i$ for each task $i$

**Deadline Monotonic**: $D_i \leq T_i$

**Rate Monotonic**: $D_i = T_i$

Futhermore, following assumptions are made:

-   Fixed-priority preemptive scheduling
-   Hard-Deadline tasks are either:
    -   *Periodic* (fixed interval)

    -   *Sporadic* (known minimum time between triggering events)
:::

#### Check Schedulability of Critical Tasks

To check if deadlines can be met (schedulable) we calculate the **response time upper bound** $R^{ub}_i$ for each task $i$. This has to be less than the task deadline $D_i$

$$
R^{ub}_i \leq D_i
$$

The tasks are ordered after priority from $i=1$ (highest priotity) and so on. Then Calculate the upper bound for every task through

$$
R_i^{ub}=\frac{C_i+\sum_{j=1}^{i-1}C_j(1-U_j)}{1-\sum_{j=1}^{i-1}U_j}
$$

$$
\begin{array}{l}
C_i \text{ worst case execution time (WCET)} \\
U_i \text{ utilisation } U_i = \frac{C_i}{T_i} \\
T_i \text{ task period}
\end{array}
$$

Thus $R_1^{ub}=C_1$ ans each lower priority task has a response time that depends on the utilisation of the tasks above it.

::: callout-caution
## Response Time Upper Bound

-   If task $R^{ub}_i \leq D_i$ checks, task is practically schedulable
-   If task fails test, there is still chance for it to work, as there've been many assumptions
-   Response times tests don't account for task interactions and os overhead
-   Tests depend on some kind of worst case execution time per task
:::

![](imagesi/paste-20.png){fig-align="center" width="40%"}

#### Estimating WCET

To estimate **W**orst **C**ase **E**xecution **T**ime, there are two basic approaches

**Static Analysis** (Analysis of the source code)

-   Relies on good processor model
-   Good for simple code & MCU
-   Difficult for complex code & MCU

**Dynamic analysis** (Measurement at runtime)

-   Common in industry
-   Must be able to exercise worst-case path
-   Simple: Toggle GPIO

### Concurrency / Gleichzeitigkeit

Tasks "logically happen" at the same time, either physically (multi-core) or through context switches (single-core). This should improve **Responsiveness**.

![Tasks of different priority in a preemptive RTOS](imagesi/paste-18.png){fig-align="center" width="50%"}

-   **Cooperative** multi-tasking: Tasks determine whether they give control back or not
-   **Preemptive** multi-tasking: A scheduler takes control of what task gets how much time and also pulls tasks from executing

::: callout-note
## Important Properties

**Safety**: Nothing bad ever happens

**Liveness**: Something useful eventually happens

**Responsiveness**: Eventually is a reasonable amount of time
:::

#### Cooperative Round-Robin

\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\begin{itemize}
    \item[+] Simple
    \item[•] No priorities
    \item[•] Worst case response = sum of all task times
    \item[•] Scheduling can be deterministic, but task periods must be harmonic
    \item[-] Must manually manage state of long-running tasks
    \item[-] Any change may alter response times
\end{itemize}

#### Preemptive: Fore-/Background

![](imagesi/paste-19.png){fig-align="center" width="40%"}

\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\begin{itemize}
    \item[+] Prioritise tasks
    \item[+] Separation of tasks and scheduling eases change
    \item[•] Worst case response = interrupt time + longest task time
    \item[•] Time-triggered scheduling deterministic, task harmonic
    \item[-] Complex task handling / 3rd-party microkernel
    \item[-] Race conditions for interrupts
    \item[-] Manual managing of long-running tasks
\end{itemize}

#### Preemptive: RTOS Impelementation

Each task is written as if it is a *single main loop*.

``` cpp
// Main Setup
#include <FreeRTOS.h>
#include <task.h>
void main() {
    xTaskCreate(BlinkTask, "BlinkA", STACK_SIZE, NULL, BLINK_PRIO, NULL);
    xTaskCreate(BlinkTask, "BlinkB", STACK_SIZE, NULL, BLINK_PRIO, NULL);
    vTaskStartScheduler();
}

// The Task
void BlinkTask(void* pvParameters) {
    while(true) {
        ledInvert();
        vTaskDelay(pdMS_TO_TICKS(500));
    }
}
```

\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\begin{itemize}
    \item[+] Prioritise tasks and responses
    \item[+] Separation of tasks and scheduling eases change
    \item[+] Long-running tasks are scheduler managed
    \item[+] Scheduling is flexible
    \item[+] Usefull features (timing-services, protocol stacks, multi-processors,...)
    \item[•] Worst-case response = interrupt time + scheduler context switch
    \item[-] Depending on 3rd-party microkernel
    \item[-] Must manage raceconditions on recourses
    \item[-] OS overhead costs recources
\end{itemize}

::: callout-warning
## Concurrency Issues

**Race Condition**: Outcome depends on timing $\rightarrow$ Occur when task modify **shared data**

![](imagesi/paste-22.png){fig-align="center" width="7cm" height="4.5cm"}

**Containment**: Keep data within a task

**Immutability**: Use unchanging data

**Atomic Data**: Only share data which can be changed atomically

**Critical Section**: Section of code must execute *atomically* (in one run)

**Synchronisation**: Concurrency Control
:::

#### Mutex (Mutual Exclusion)

Only one task can take / lock the mutex at a time. Other task trying to acquire the mutex are blocked until the mutex is released. A mutex can only be ***released** by the task that **acquired** it*.

If two or more tasks **share a recource**, use a mutex for protection.

``` cpp
#include <semphr.h>
SemaphoreHandle_t mutex = xSemaphoreCreateMutex();
...
// in a task
for (int32_t i = 0; i < 1000000; i++) {
    xSemaphoreTake(mutex, portMAX_DELAY);
    counter = counter + 1;
    xSemaphoreGive(mutex);
}
```

::: callout-note
## Encapsulate Synchronization

-   Avoid scattering mutexes around the code
-   Prevent client tasks of accessing mutexes directly
-   Ensure only a single mutex is hold at a time

``` cpp
// counter_manipultor.c
static SemaphoreHandle_t mutex;
static int32_t counter = 0;

void counterAdd(int32_t value) {
    xSemaphoreTake(mutex, portMAX_DELAY);
    counter = counter + value;
    xSemaphoreGive(mutex);
}
int32_t counterGetValue() {
    xSemaphoreTake(mutex, portMAX_DELAY);
    int32_t local = counter;
    xSemaphoreGive(mutex);
    return local;
}

// counter_task.c
void CounterTask(void* pvParameters) {
    for (int32_t i = 0; i < 100; i++) {
        counterAdd(1);
    }
    int32_t final = counterGetValue();
    printf“(%”d, final);
    vTaskSuspend(NULL);
}
```
:::

#### Semaphore

A semaphore can be ***given** by any task*. To receive and wait for a signal use `xSemaphoreTake(...)`.

If two or more tasks need to **coordinate actions**, use a semaphore to send signals.

``` cpp
SemaphoreHandle_t signal = xSemaphoreCreateBinary();
void TaskA(void* pvParameters) {
    for (;;) {
        printf("Ready!");
        xSemaphoreGive(signal);
        vTaskSuspend(NULL);
    }
}
void TaskB(void* pvParameters) {
    for (;;) {
        xSemaphoreTake(signal, portMAX_DELAY);
        printf("Go!");
        vTaskSuspend(NULL);
    }
}
```

#### Task Notification

Task notifications are FreeRTOS specific and offer a *light weight* alternative to a semaphore.

#### Queues

Used to send data from one task to the other. Data is written to a queue **as copy**.

``` cpp
// create queue
#include <queue.h>
QueueHandle_t msgQueue = xQueueCreate(QUEUE_SIZE, sizeof(msg_t));

// send a message
xQueueSend(msgQueue, &toSend, 0);

// receive a message
xQueueReceive(msgQueue, &received, portMAX_DELAY);
```

`xQueueReceive(..., portMAX_DELAY)` is blocking until something is put into the queue. The time can be adjusted by the last argument, usually `portMAX_DELAY`, veeeeery long.

#### Deadlock

.

![](imagesi/paste-26.png)

We can also design tasks to only block in one place and thus deadlocks are less likely

``` cpp
void Task(void* pvParameters) {
    for (;;) {
        xQueueReceive(...); // single block
        switch (received.msgType) {
            case MSG_A: // Handle A events 
            case MSG_B: // Handle B events 
            case TIMER_1: // Handle timer 
            case default: // Assert?
        }
    }
}
```

Or use a structure which prevents deadlocks generally

![](imagesi/paste-28.png){fig-align="center" width="8cm"}

Use a **Pipeline** to minimise circular dependencies and a **Client-Server** structure to restrict the directionality of connections.

#### Priority Inversion

.

![](imagesi/paste-25.png)

## Resources

::: callout-note
## Ausgangslage TIVA

32KB RAM, 256KB ROM, Low-Power

FreeRTOS needs: 5-10KB ROM, RAM: 236bytes (scheduler), 76bytes + storage size (queue), 60 bytes + stack size (task)
:::

::: callout-warning
## Premature optimization is the root of all evil.

-   Don't optimize before you know your constraints.

-   Don't waste time optimizing things that won't help.

-   Don't add complexity you don't need
:::

![](imagesi/paste-41.png)

### Choose

#### Get the memory map (static)

``` cpp
// add this to CMake
set(CMAKE_EXE_LINKER_FLAGS
    "-T\"${SCRIPTS_DIR}/link.ld\" \
    -Xlinker -Map=${CMAKE_CURRENT_BINARY_DIR}/%
    ")

// output
... 
.text 0x00000470 0x280 accl_manager.c.obj
      0x00000470       acclInit
      0x00000488       acclProcess
...
.data 0x2000005c 0x4   __atexit_dummy
      0x20000060 0x4   __atexit_recursive_mutex
...
.bss 0x2000081c 0x10 accl_manager.c.obj
...
    
```

The distance between two functions equals their size (mostly).

-   **.text** - my code, vector table plus constants

-   **.data** - initialized variables and counts for RAM and FLASH *(linker allocates data in FLASH which is then copied from ROM to RAM on startup*

-   **.bss** - uninitialized data in RAM *(initialized with zero on startup)*

::: callout-important
## Dynamic Allocation Analysis

Dynamically allocated memory is difficult to analyse statically and dynamic analysis is really hard on embedded systems.

Usually measure the dynamic memory allocation with FreeRTOS macros `traceMALLOC`and `traceFREE`. They keep track on the allocated memory.

**Most important**: Detect Memory Leaks! (MALLOC but no FREE)
:::

#### Use `objdump -d` to disassemble executable

![](imagesi/paste-44.png){fig-align="center" width="7cm" height="4.5cm"}

#### Profiling

-   Use GPIO and oscilloscope to profile how long certain parts of a function run
-   Use a DIY sampling profiler

``` cpp
(gdb) continue Continuing.
^C Program received signal SIGINT, Interrupt. prvIdleTask (pvParameters=0x0 <vPortValidateInterruptPriority>)
    at FreeRTOS/Source/tasks.c:3487
3487                vApplicationIdleHook();
```

### Change

-   Reduce RAM requirements by removing *concurrency*
-   Trade *time* for *space*
-   Increase *coupling* by sharing data
-   Add structural or computational *complexity* to allow reusing limited memory
-   Turn off unused peripherals (energy)
-   Use lower clock rates or put MCU to sleep (energy)

#### Flyweight pattern

Task refers to same immutable data over and over again

![](imagesi/paste-42.png){fig-align="center" width="7cm"}

#### Memory Pool pattern

Prebuilt datapool for mutable data $\rightarrow$ Control over size and avoids memory fragmentation

![](imagesi/paste-43.png){fig-align="center" width="7cm"}

#### Compiler and Linker help

``` cpp
// Optimize for size 
gcc –Os <your file>.c
// Use link-time optimizations 
gcc -flto
// Use linker to remove unused code and data 
gcc -ffunction-sections -fdata-sections 
Link with --gc-sections
// Optimize for speed 
gcc –O2 <your file>.c 
gcc –O3 <your file>.c
```

#### Use better algorithms

.

![](imagesi/paste-45.png)

#### Write better algorithms

.

![](imagesi/paste-46.png)

#### Cache Data

.

![](imagesi/paste-47.png){fig-align="center" width="7cm"}

#### More

-   Minimize data passing by **passing by reference**
-   **Datacompression** with Differencing (store difference of two values in sequence), or Run length encoding
-   Task notifications instead of queues
-   Limit scope of local variables
-   **Reduce Overhead** through removing layers
-   Reduce sampling rate

Engineering in the face of uncertainty:

1.  Use large margins (e.g. RAM 200%-300%)
2.  Establish resource budgets for major elements
3.  Estimate resource usage for each element early
4.  Track resource use against budget (e.g. dashboard)
5.  Finish with 20%-50% margin for future growth

### Performance

## Preemptive Debugging

![](imagesi/paste-39.png){fig-align="center" width="6cm" height="3.7cm"}

``` cpp
float squareRoot(float x) {
    // Require:
    // Failure equals happened before function gets called
    assert(x >= 0);
    y = ... ;
    // Ensure:
    // Failure equals error in this function
    assert((y*y) == x);
    return y;
}
```

With FreeRTOS `configAssert( x )` can be called. The behaviour is user dependent (standard `while(true);` $\rightarrow$ Fail-Safe.

``` cpp
/* Custom implementation */
void vAssertCalled( const char * pcFile, unsigned long ulLine ) {
 (void)pcFile; // unused
 (void)ulLine; // unused
 while (true); // LED, EEPROM,...
}
```

**D**on't **R**epeat **Y**ourself - **DRY**

![](imagesi/paste-40.png)

::: callout-warning
## Don't do this

``` cpp
ASSERT(kbPress != 'j');
```

Avoid asserting **expected errors** $\rightarrow$ Handle directly in code

``` cpp
ASSERT(i++ < 5);
```

Avoid **Sideeffects**
:::

### Static Analysis

Happens **at compile-time**:

``` cpp
_Static_assert(BLINK_STACK_SIZE > configMINIMAL_STACK_SIZE, "Stack size too small");
```

Use a framework like [Frama-C](https://frama-c.com/):

``` cpp
/*@
    requires y > 0
    ensures \result > y
*/
int makeLarger(int y) {
    int x = y*2;
    return x;
}
```

## Security

![](imagesi/paste-48.png)

::: callout-note
## C is not a "safe" language

Array overflow is not prevented, you could access the `state` variable in the following example:

![](imagesi/paste-49.png){fig-align="center" width="6cm" height="3.5cm"}
:::

### Security failures are often design failures

Use the checklist **STRIDE** and design a architecture to avoid running into problems

-   **S**poofing

-   **T**ampering

-   **R**epudiation

-   **I**nformation Disclosure

-   **D**enial of Service

-   **E**levation of Privilige

![](imagesi/paste-53.png){fig-align="center" width="8cm"}

### Evaluate Threads

This is time consuming $\rightarrow$ **in Projektmanagement beachten**

<https://emb3d.mitre.org/>

![](imagesi/paste-54.png){fig-align="center" width="8cm"}

### Follow "secure" coding standards

<https://securecoding.cert.org/>

![](imagesi/paste-50.png){fig-align="center" width="8cm" height="3.7cm"}

### Use static analysis to find problems

``` cpp
// Turn on all warnings 
gcc –Wall -Wextra <your file>.c
// Use static analyzer 
gcc –fanalyzer <your file>.c
```

### Consider exceptional CHILDREN

-   **C**omputation
-   **H**ardware
    -   Transient Faults
    -   Memory Corruption
-   **I**/O
    -   Running out of file space?
-   **L**ibrary
    -   Handle error returns
-   **D**ata input
    -   Buffer input overflows
    -   More / Less data than expected
-   **R**aces and deadlocks
-   **E**xternal user
    -   Wrong, Late, Other input
-   **N**ull pointer and memory

# Testing

Testing is for *Finding Bugs*, *Reduce risk to user **and** business*, *reduce development costs*, *keep code clean*, *improve performance* and to *verify that **requirements are met***. There are different test which can be performed:

-   *Unit Testing*: Verify behaviour of individual units (modules)
-   *Integration Testing*: Ensure that units work together as intended
-   *System Testing*: Test **end-to-end** functionality of application
-   *Acceptance Testing*: Verify that the requirements are met (whole system)
-   *Performance Testing*: Evalutate performance metrics (e.g. execution time)
-   *Smoke Testing*: Quick test to ensure major features are working

To make testing efficient, we implement automatic testing routines. They act as a **live** documentation. Allows for **refactoring with confidence**.

## Unit Test

A good test case checks **one behaviour** under **one condition**, this makes it easier to localise errors.

``` cpp
void test_single_element_in_single_element_out(void) {
    // Arrange: given buffer has a single element
    writeCircBuf(&buff, 11);
    // Act: when buffer is read
    uint32_t value = readCircBuf(&buff);
    // Assert: then the same value is returned
    TEST_ASSERT_EQUAL(11, value);
}
```

::: callout-note
## Testing Frameworks

-   [Unit Test Framework Unity](https://www.throwtheswitch.org/unity)
-   [Test Double Framework fff](https://github.com/meekrosoft/fff)
:::

### Unit Test with Collaborators

![](imagesi/paste-13.png)

### Test Doubles

Implement test doubles through the fake function framework ([fff](https://github.com/meekrosoft/fff)). There are different variations of test doubles:

**Stub**: Specify a return value - *Arrange*

``` cpp
// Set single return value
i2c_hal_register_fake.return_val = true;
// Set return sequence
uint32_t myReturnVals[3] = { 3, 7, 9 };
SET_RETURN_SEQ(readCircBuf, myReturnVals, 3);
```

**Spy**: Capture Parameters - *Arrange* / *Assert*

``` cpp
// Arrange, e.g. get passed function
adc_hal_register(ADC_ID_1, dummy_callback);
void (*isr) (void) = ADCIntRegister_fake.arg2_val;
// Assert Parameter
TEST_ASSERT_EQUAL(3, ADCSequenceDataGet_fake.arg1_val);
```

**Mock**: Can act as a *Stub*, *Spy*, and much more (from fff). Implemented as follows:

``` cpp
// in some_mock.h
VALUE_FUNC(uint32_t *, initCircBuf, circBuf_t *, uint32_t);
VOID_FUNC(writeCircBuf, circBuf_t *, uint32_t);
```

**Fake**: Provide a custom fake function - *Arrange*

``` cpp
// Define Fake Function
int32_t ADCSequenceDataGet_fake_adc_value(uint32_t arg0, uint32_t arg1, uint32_t *arg2) {
    *arg2 = FAKE_ADC_VALUE;
    return 0;
}
// Apply Fake Function - Arrange
ADCSequenceDataGet_fake.custom_fake = ADCSequenceDataGet_fake_adc_value;
```

### Continuous Integration

*CI* is used to automate the integration of code changes. These are automated scripts running all the tests. This is usually implemented in the code hoster (e.g. *GitLab*) and is executed after every push. It also runs before every merge and **blocks a merge** if one of the tests fails.

## Higher Level Testing

Unit tests only verify small elements of a system in isolation.

### Automated Acceptance Testing

-   Verifys system requirements
-   Live documentation of high-level requirements
-   Understandify behaviour
-   Acceptance test pass $\rightarrow$ requirement met
-   Written by PM or QA ( $\approx$ customer)
-   Written in natural scripting language
-   Non-Technical stakeholder in the loop
-   Called: **B**ehaviour-**D**riven **D**evelopment **BDD**

### Automated System Tests

**Hardware Simulation**: Developed on PC, no need to know specific hardware implementation yet, limitations with hardware peripherals.

**Hardware Emulation**: Emulate processor on PC, needs resources for emulator. Tools: QUEMU

**Hardware in the Loop**: Runs on target, test scripts on a test enclosure to manipulate hardware, expensive setup. Tools: NI DAQ, Labview

![](imagesi/paste-30.png){fig-align="center" width="8cm"}

### Manual Testing

Sometimes automated test setups are more expensive.

Manual testing can involve **user interaction**, **Debugger**, **direct Signal Probing** (Oscilloscope, Multimeter, Logic Analyzer).

## Test Driven Design TDD

![](imagesi/paste-29.png)

Applying TDD through writing *unit tests* during development, benefits:

-   **Reduce Debug Time**: small feedback loop
-   **Courage to make changes**: tested code is changeable code
-   **Tests are Reliable**: high level of coverage
-   **Good Architecture**: Writing test implies decoupling

For each new unit:

1.  Come up with a **set of requirements**
2.  Generate a **rough test list**
3.  Implement unit by going through the list with **TDD**
4.  Tick off, remove, or add items to/from the list in the process

Use **ZOM** to come up with tests:

-   **Z**ero case(s): simplest scenario $\rightarrow$ build interface
-   **O**ne case: simplest scenario to transition from **Z**ero to **O**ne
-   **M**any cases: generalise design, each test case adds a scenario

# Computer History

## Moor's Speculation

The *prediction* of Gordon Moore states "doubling the number of components on a IC each year". This is also largely true.

![](imagesi/paste-58.png)

The exponential growth turned out to be true, for how much longer?

## Early History of Digital Computers

The early history of digital computers and microprocessors highlights the progression from large, power-hungry machines to more efficient, smaller designs:

-   **1950s**: The largest computer, IBM AN/FSQ-7, used 55,000 vacuum tubes and consumed 3 MW of power.
-   **1964**: DEC introduced the first minicomputer, the PDP-8, followed by the PDP-11, which played a key role in developing Unix and C.
-   **Intel 4004 (1971)**: The first microprocessor with a 4-bit CPU and 2300 transistors, originally designed for calculators.
-   **Intel 8008 (1972)**: A more advanced 8-bit CPU with 5000 transistors.
-   **Intel 8080 (1974)**: Improved performance (10x over 8008) and used in the first PC (Altair).
-   **Texas Instruments TMS1000 (1974)**: First microcontroller, used in calculators and appliances.
-   **MOS Technology 6502 (1975)**: Popular in early PCs (Commodore, Apple, Atari) for \$25.
-   **Intel 8086/8088 (1978)**: Intel’s first 16-bit processor, chosen for the IBM PC despite flaws.
-   **Motorola 68000 (1979)**: A 32-bit processor used in the Apple Macintosh.
-   **Intel 386 (1985)**: Introduced linear addressing and virtual memory support, popular in PCs by 1990.

::: callout-note
## Why x86?

The **IBM PC** is the first personal computer, the engineers wanted to use the *Motorola 68000* chip, but management choose the *Intel 8088*, that's the dawn of Intels dominance and thus the dominance of the *x86-Architechture*

![](imagesi/paste-59.png){fig-align="center" width="7cm"}
:::

# Fundamentals of Microprocessors and Architectures

## Microprocessor architectures

**Flynn's Classification** processor architectures into subgroups

### **SISD** - Single Instruction / Single Data

Processors with a SISD-Architecture are **Uniprocessors (von Neumann)**, they consist of a single processor. They are partitioned into \*input devices, output devices, memory, ALU and control unit

#### Princeton

Memory stores *data and instructions*, thus the **von Neumann Bottleneck** appears. There is also the risk of **accessing data as instructions** and vice verca. This can stall the CPU and is mitigated through memory management units (MMU) in modern processors.

![](imagesi/paste-60.png){fig-align="center" width="8cm"}

#### Harvard

The memory is seperated as data / instruction. This adds security, but harder for the programmer (What memory space does a pointer point to?).

![](imagesi/paste-61.png){fig-align="center" width="8cm"}

#### Modified Harvard

There are separate instruction and databuses, but share the same memory space. Application: **DSP**

![](imagesi/paste-65.png){fig-align="center" width="8cm"}

#### Hybrid

There is a single memory space and data/insn-bus, but to gain speed, caches are used to mimic a harvard architecture.

![](imagesi/paste-62.png){fig-align="center" width="8cm" height="4.3cm"}

### **SIMD** - Single Instruction / Multiple Data

Typical SIMD-Processors are **Array-/Vector-Processors** and are often used in *GPUs* or *special CPUs*. Each processor simultaneously performs the same instruction on different data.

::: callout-note
## SIMD Instructions

TO operate on **vector registers**...

-   *... ARM* Processors have **NEON** instructions
-   *... Intel* Processors have **SEE** instructions
:::

### **MIMD** - Multiple Instruction / Multiple Data

MIMD-Processors are multi-core processors, with several cores, often accessing the same memory. This is done to improve computer performance through **parallelism**.

#### **SMP** - Symmetrical Multiprocessor

Few identical processors share a common memory. Caches are used to mitigate the *von Neumann Bottleneck*.

![](imagesi/paste-63.png){fig-align="center" width="9cm" height="3.1cm"}

#### **MPP** - Massively Parallel Processor

Many processors using distributed memory and communication through a network *(many topologies, e.g., star, ring, ...).*

![](imagesi/paste-64.png){fig-align="center" width="9cm"}

### Accessing Program Memory on AVR

AVR is a manufacturer who uses the *Harvard*-Architecture. To access the program memory (flash) the `PROGMEM` macro from `avr/pgmspace` is used. *(support of that is introduced in GCC 4.8)*

``` c
#include <avr/pgmspace.h>

// for flash access
PROGMEM const char flash_str[] = "Hello world";
char flash_read_byte (const char *p) {
    return pgm_read_byte(p);
}

/* translates to following assembly */
flash_read_byte:
    movw r30 , r24
    lpm r24 , Z    // load program memory
    ret

// for SRAM access
const char sram_str[] = "Hello world";
char sram_read_byte (const char *p) {
    return *p;
}

/* translates to following assembly */
sram_read_byte:
    movw r30 , r24 
    ld r24 , Z
    ret
```

## Instruction Set Architectures (ISA)

The *instruction set architecture* (ISA) is the assembly language of a specific processor. The ISA includes things such as:

-   Instruction Set (NOP, BRA, LOAD, ADD, ...)
-   Data Types (u/s-int, float, addresses)
-   Registers
    -   PC: programm counter
    -   SP: stack pointer
    -   R0, R1, R2, ... : general purpose registers
    -   W, Z, V, C: status registers
-   Addreassing nodes, for accessing memory

### Register Transfer Language (RTL)

Describes how a cpu instruction behaves

![](imagesi/paste-66.png){fig-align="center" width="9cm"}

### Stack ISA

Stack ISAs operands are pushed onto a stack before operators are applied. Source operands are popped off the stack, and destination operands are pushed back onto it, resulting in very short instructions. However, the stack can become a bottleneck, as it is not randomly addressable.

![](imagesi/paste-67.png){fig-align="center" width="10cm"}

### Accumulator ISA

Accumulator ISAs use a single accumulator register for storing the results of all ALU operations, leading to short instruction lengths where only one operand is specified. Memory traffic is high since the accumulator is the primary storage, but this design was popular in early microprocessors when memory and CPU speeds were comparable.

![](imagesi/paste-68.png){fig-align="center" width="10cm"}

### General Purpose Register

::: callout-caution
## Issue with frequent memory access

Because the speed of **DRAM** didn't keep up with the speed of **CPUs**, memory is a bottleneck of performance.

![](imagesi/Memory-Access-vs-CPU-Speed.png){fig-align="center" width="7cm"}
:::

Most high-performance computers use general-purpose register ISAs because registers are faster than memory. These ISAs have many registers, explicit operands, and longer instructions. Load/store ISAs, where ALU instructions only operate on registers, are popular despite requiring more instructions because they are simpler and more suited to pipelining. ISAs can be classified into memory-memory, register-memory, and register-register, based on how many memory operands an ALU instruction can have.

![](imagesi/paste-69.png){fig-align="center" width="9cm" height="3.4cm"}

![](imagesi/paste-70.png){fig-align="center" width="9cm"}

## Instruction encoding

![](imagesi/paste-71.png){fig-align="center" width="6cm"}

::: callout-note
## ALU flags

| Z : result is *zero*
| N : result is *negative*
| V : result has *signed overflow*
| C : result has *unsigned overflow* **carry**
:::

*Opcodes* encode the operations to perform. There are different kind of opcodes, depending on how many operands are involved:

**Binary ALU instructions**: Two source operands

![](imagesi/paste-72.png){width="5cm" height="1.1cm"}

**Unary ALU instructions**: One source operand

![](imagesi/paste-73.png){width="5cm" height="1.1cm"}

**No Operand instruction**: No operand, doens't use ALU

![](imagesi/paste-74.png){width="4cm" height="0.7cm"}

### Binary ALU Instruction Encoding

![](imagesi/paste-75.png){fig-align="center" width="8cm"}

#### Operands

Operands can be a **Register**, a **Constant**, or at a **Memory Location**.

![](imagesi/paste-77.png){width="7cm"}

::: callout-warning
## Length of Instructions

If you want to load large constants (32-bit words), you won't have enough space in your instruction encoding. There are two solutions:

**CISC**: Variable length instructions

This introduces the issue of different instruction having different lengths, and thus needing more (specific) hardware.

![](imagesi/paste-78.png){fig-align="center" width="8cm" height="1.2cm"}

**RISC**: Fixed length instructions

This is great for $\textcolor{red}{\text{Pipelining}}$, and thus makes a processor faster. To fix the size issue we can use:

1.  Synthesize large constants using multiple insn\
    ![](imagesi/paste-80.png){width="7cm" height="1.2cm"}
2.  Load a constant from literal pool\
    ![](imagesi/paste-82.png){width="8cm"}
:::

## CISC to RISC

::: callout-important
## CISC issues

-   Initially ISAs were designed for easy assembly language programming

-   This introduced many...

    -   ... datatypes: $\tiny\textcolor{orange}{\text{byte/short/int/float/double/BCD/bit fields}}$

    -   ... instructions: $\tiny\textcolor{orange}{\text{crc (cyclic redundancy check)/polynomial extension/linked list insertion/...}}$

    -   ... addressing modes: $\tiny\textcolor{orange}{\text{R0<-M[3]/R0<-M[R1]/R0<-M[R1+R2]/R0<-M[M[R1+R2]]}}$
:::

### Computer Performance Measures

To measure the compute performance, execute a benchmark and take the time:

![](imagesi/paste-83.png){fig-align="center" width="9cm"}

To minimise program execution time, reduce the instruction count, reduce the CPI (Clocks per Instruction), or increase the Clock rate.

| *Clock Rate*: Hardware technology and processor organisation
| *CPI*: Processor organisation and instruction set architecture
| *Instruction Count*: Instruction set architecture and compiler technology

::: callout-warning
## Misleading MIPS

**M**illion **I**nstructions **P**er **S**econd is a missleading measure, as a processor without a FPU can have higher MIPS (simpler architecture), but takes longer to calculate. Furthermore, it doesn't take *chip cost* and *power consumption* into account.

$$
\text{MIPS}=\frac{\text{Clock Rate}}{\text{CPI}*10^6}=\frac{\text{Instruction Count}}{\text{Program Execution Time}}
$$
:::

### Amdahl's Rule

**Make the common case fast**

To compare performance improvements, we need the *speedup* and *utilisation*

$$
\text{Exec Time New}=\text{Exec Time Old}\left[(1-\text{Utilisation})+\frac{\text{Utilisation}}{\text{Speedup}}\right]
$$

$$
\text{Speedup Overall}=\frac{\text{Exec Time Old}}{\text{Exec Time New}}=\frac{1}{(1-\text{Utilisation})+\frac{\text{Utilisation}}{\text{Speedup}}}
$$

![](imagesi/paste-84.png){fig-align="center" width="9cm" height="1.5cm"}

With this we have a absolute number of the overall speedup. RISC was developed with this in mind, which lead to a overall improvement, even though the instruction count went up

![](imagesi/paste-85.png){fig-align="center" width="8cm" height="1.8cm"}

![General Purpose Register CPU](imagesi/paste-86.png){fig-align="center" width="8cm"}

# Pipeline and Parallelism

## Computer pipelining

**For bigger throughput, separate stages should take the same amount of time.**

A CPU needs the basic 5 stages (modern CPUs have way more):

1.  *Fetch*: fetches insn from memory
2.  *Decode*: Determine opcode & operands
3.  *Read*: Reads source operands
4.  *Execute*: ALU performs operation
5.  *Write*: ALU result is written back to register

A 5 stage pipeline provides a CPI=1, but a latency of 5 clock cycles.

![](imagesi/paste-87.png){fig-align="center" width="9cm"}

## Pipeline Hazards

There are **Resource**-, **Data**-, and **Control**-Hazards.

### Resource Hazards

Occur when two stages need the same resource (e.g. memory, ALU). To mitigate this issue, the pipeline is **stalled**, which introduces a *pipeline bubble* and raises the CPI.

![](imagesi/paste-89.png){fig-align="center" width="9cm"}

A example for this hazard is the *division operation*, as it requires the $N$ ALU-operations for $N$-bit operands.

### Data Hazards

#### RAW, **R**ead **A**fter **W**rite

$I1$ & $I2$ have a **true dependency** on `R0`.

![](imagesi/paste-88.png){fig-align="center" width="8cm"}

| *Problem*: `R0` is written in stage 5 for $I2$, but read in stage 3 for $I2$
| *Solution*: **Stalling** pipeline, until `R0` is written by $I1$

#### WAR, **W**rite **A**fter **R**ead

$I1$ & $I2$ have a **anti-dependency** on `R0`.

![](imagesi/paste-90.png){fig-align="center" width="8cm" height="0.7cm"}

*Potential Problem*: If $I2$ writes `R0` before $I1$ reads\
-\> this doesn't happen in the usual case\
-\> can occur with out of order execution (**superscalar**)

#### WAW, **W**rite **A**fter **W**rite

$I1$ & $I2$ have a **output dependency** on `R0`.

![](imagesi/paste-91.png){fig-align="center" width="8cm" height="0.9cm"}

This is very **uncommon**, only with badly optimised out of order execution.

### Resource and Data hazard avoidance

**Static Hazard Detection** (compiler)

1.  Place useful instructions between $I1$ and $I2$ , that don't alter the program
2.  Otherwise stall the pipeline with `NOP`

**Dynamic Hazard Detection** (CPU)

1.  Place useful instructions between $I1$ and $I2$ , that don't alter the program (out-of-order processors)
2.  Otherwise stall the pipeline using a *pipeline interlock*

### Control Hazards

Occur due to *changes in the program flow* ($\textcolor{orange}{\text{branches/jumps/interrupts/function calls}}$), which change the $PC$ often unpredictably.

![](imagesi/paste-92.png){fig-align="center" width="8cm"}

The hazard occurs, because the CPU doesn't know what to fetch after the branching `BNE`.

#### Control Hazard Avoidance

To avoid a control hazard, the cpu can:

1.  **Stall** the pipeline until it's known what path is taken (CPI goes up!)\
    ![](imagesi/paste-93.png){width="7cm"}
2.  **Assume** the pipeline increments normally -\> process after branch -\> **flush** pipeline if wrong\
    ![](imagesi/paste-94.png){width="7cm"}
3.  **Predict** which way the branch goes (**flush** if wrong) -\> **Speculative Execution**
    i.  *prediction on heuristics*: backwards branch is more likely to be taken (loops)
    ii. *record previous branch prediction*: CPUs have branch prediction HW

#### Branch Prediction

CPUs have a bit that indicate which way the branch previously went. They're assuming it goes the same way next time. This is a good assumption for e.g. `for(i=0;i<100;i++){ }`. Modern CPUs use multiple branch prediction bits and the PC is used as a hash key.

#### Conditional Move Instructions

To avoid frequent flushing of pipelines, a conditional move instruction was introduced, which moves depending on the result of the previous insn.

![](imagesi/paste-95.png){fig-align="center" width="7cm"}

#### Interrupts

-   Tricky since PC changes unpredictably
-   When interrupt occurs:
    1.  pipeline needs to be flushed
    2.  pipeline restarts with ISR code
-   When interrupt finishes
    3.  pipeline restarts at the point the program was interrupted

#### Program to reduce control hazards

With modern CPUs and compilers (if no aliasing \[nicht abtasttheorem\] is detected), the difference in performance will be minimal. A technique is the **loop unswitching**. Loop optimisation, should start from the inner to the outer loops.

![](imagesi/paste-96.png){fig-align="center" width="8cm"}

## Instruction level parallelism

The goal is to reduce

$$
\text{CPI}<1
$$

This requires multiple instructions executed at same time. For this multiple **Functional Units** (execution units) are used

![](imagesi/paste-97.png){fig-align="center" width="8cm"}

### VLIW Processor

**V**ery **L**ong **I**nstruction **W**ord processor explicitly state what each execution unit does. Thus the instructions are much longer

![](imagesi/paste-99.png){fig-align="center" width="8cm" height="0.6cm"}

This relies on the compiler and it can also happen that not every functional unit is utilised. For this `NOP`s have to be inserted, the CPI goes up and memory is wasted.

![](imagesi/paste-100.png){fig-align="center" width="8cm" height="0.6cm"}

### Superscalar Processor

This CPU fetches and decodes a standard stream of instructions and dispatches them to the appropriate functional unit. To make efficient use of this, a *instruction cache* is used. From the $n$ different instructions can be fetched, decoded and dispatched per cycle.

$n$**-way superscalar** $\qquad\text{CPI}=\frac1{n}$

![](images/paste-1.png){fig-align="center" width="6cm"}

#### Data Dependencies

When running several instructions in parallel introduces the issue of data hazards

![](images/paste-3.png){fig-align="center" width="8cm" height="1.5cm"}

-   `R3`: cannot execute $I2$ until $I1$ is finished
-   `R4`: cannot execute $I3$ before $I2$ is finished

#### Register Renaming

CPUs have additional registers to use in place, which the programmer has no access to. Any followup instructions using the `R4` result become dependent on `R24`.

![](images/paste-4.png){fig-align="center" width="6cm" height="1.1cm"}

#### Tomasulo's Algorithm

Tomasulo's algorithm schedules instructions on superscalar CPUs.\
It handles RAW, WAR, WAW hazards.\
It introduces and handles *out-of-order-execution\
*It uses **reservation stations** (RS) to hold instructions before they are executed

![](images/paste-5.png){fig-align="center" width="9cm"}

1.  While RS is not available: *Stall Pipeline*
2.  Copy instructions and available operands into RS
3.  Mark destination register as unavailable
4.  If RS has all operands and a suitable FU available: issue instruction to FU and free RS
5.  Copy the result to destination register and any waiting RS
6.  Mark destination register as available

The **availability**-register states if the operand is available for further processing, or if a operation is performed for it right now. (avoid RAW hazard)

Instructions sit in a RS until a FU is available and all register source operands are known.

WAW & WAR hazards are avoided by copying register values into RS

Reservation stations have a high cost in comparators, needed to compare all results returned from processing units with all stored addresses.

### Multithreading CPU

To maximise FU usage to get the lowest CPI we allow multiple threads to run in parallel, sharing the CPU. For this we need:

-   Two program counters, for each thread one
-   Two sets of registers, for each thread one
-   Threads share functional units and reservation stations

This results in better utilisation of the functional units (e.g.: one thread does integer (gcc), the other floating point (simulations) / one thread executes, one thread is waiting for memory)

# Memory Systems and Optimization

Because CPUs increased in speed at a faster rate than DRAM, **Memory Caches** are introduced to mitigate a memory bottleneck.

## Cache memory systems

![](images/paste-6.png){fig-align="center" width="9cm" height="2.7cm"}

![](images/paste-7.png){fig-align="center" width="8cm"}

-   **L1**-cache: split in insn and data, to mitigate von neumann-bottleneck
-   **L2**&**L3**-cache: unified data caches for one or several cores

### Average Memory Access Time

The goal of a good memory hierarchy is to keep the total memory cost down whilst still trying to keep the average memory access time fast. This is measured by the *Average Memory Access Time*:

$$
\begin{aligned}
\text{Average }&\text{Memory Access Time} \\ & = \text{Hit Time} * \text{Hit Ratio} + \text{Miss Time} * (1 - \text{Hit Ratio}) \\
& = \text{Hit Time} + \text{Miss Penalty} * (1 - \text{Hit Ratio})
\end{aligned}
$$

-   Hit Time: Memory access time for data in cache
-   Hit Ratio ($\textcolor{orange}{\text{Typically }85-95\%}$) : Proportion of memory access that is cache

$$
\text{Hit Ratio (aka Hit Rate)}=\frac{\text{Cache Hits}}{\text{Total Memory Requests}}
$$

::: callout-note
## Cache Parameters

From the cache parameters it's obvious, that frequent reading from the system memory should be avoided if possible

![](images/paste-11.png){fig-align="center" width="8cm"}
:::

### Cache Locality

-   **Temporal** locality: Recently read locations are often re-read *(loops)*
-   **Spatial** locality: Likely to read memory *close* to previously read location *(iterate over array)*

::: callout-caution
## Iterating over multi dimensional arrays

When looping over multidimensional arrays, the innermost loop should always loop over data next to each other.

``` c
/* Good data cache usage */
double matsum1(const double *mat, unsigned int rows, unsigned int cols) {
    unsigned int i, j;
    double total = 0.0;
    for (j = 0; j < cols; j++)
        for (i = 0; i < rows; i++)
            total += mat[j * rows + i];
    return total;
}

/* Poor data cache usage */
double matsum2(const double *mat, unsigned int rows, unsigned int cols) {
    unsigned int i, j;
    double total = 0.0;
    for (i = 0; i < rows; i++)
        for (j = 0; j < cols; j++)
            total += mat[j * rows + i];
    return total;
}
```
:::

## Cache architectures

### Look Aside

The **Look Aside** cache connects the CPU directly to all memory, and thus requests it from all memory. If the data is in cache, the *acces cycle is terminated*.

![](images/paste-8.png){fig-align="center" width="6cm" height="3.1cm"}

### Look Through

In a **Look Through** configuration, the CPU is only connected to the L1 cache and the L1 cache acts as the master for the next higher memory access. There is *less traffic* on the main system bus, but the hardware is more complicated and it's slower.

![](images/paste-9.png){fig-align="center" width="6cm" height="1.4cm"}

### Multicore Cache Architecture

The stages of **L1/L2/L3**-caches are all *look through*.

**Cache Coherence**: All caches must be kept consistent (The same data spread over several caches/memory locations must stay the same).

![](images/paste-10.png){fig-align="center" width="10cm"}

::: callout-tip
## Monitoring Tools

To monitor cache acces behaviour tools like

-   **Valgrind** - Finding memory leaks or read from uninitialised memory
-   **Cachegrind** - Analysis of cache & branch prediction

**Cachegrind**: Uses just in time translation and runs the modified program (slower). It records information to a file which can be analysed afterwards.

``` c
valgrind --tool=cachegrind -v ls
```

![](images/paste-12.png)
:::

### Cache Coherence

-   Chaches store a local copy of main memory, but must be consistent (over all caches)
-   If both caches have a copy of the same memory location and
    1.  cache A is modified
    2.  cache B can be $\textcolor{orange}{\text{updated}}$ or $\textcolor{orange}{\text{invalidated}}$
        -   if invalidated, the cache is updated when the CPU wants to read the previously cached location

#### Bus Snooping

Each cache monitors all the other caches bus traffic to see if another cache changed.

![](images/paste-13.png){fig-align="center" width="8cm" height="2.9cm"}

**Problem**: This doesn't scale well for many processors and caches.

#### Directory Based Coherence

Directory holds an entry for each memory line to say which caches have a copy. The dirty bit then indicates if the memory block is in a modified state and has to be updated before working with it.

![](images/paste-14.png){fig-align="center" width="8cm"}

## Cache Organisation

-   Cache needs to be **fast**
-   Need a fast way to determine if a memory location is in cache

If use **LUT**, assume 32-bit addr, 1 bit per LUT-entry: *not feasable*

$$
2^{32}\cdot 1bit = 4Gbits
$$

Use **Hashing Techniques** with a cache directory (SRAM).

### Direct Mapped (DM) Cache

Cache controllers must quickly determine if memory is cached. For this the (actual) memory address is split into three fields:

| Line Offset Bits = $\log_2(\text{Line Size)}$
| Line Index Bits = $\log_2(\text{Cache Size})-\log_2(\text{Line Size})$
| Tag Address Bits = $\text{Address Size Bits}-\log_2(\text{Cache Size})$

![](images/paste-16.png){fig-align="center" width="8cm"}

-   The **Line Index** is used to index the cache and the cache directory.
-   The **Cache Directory** stores the last cached **Tag Address** and outputs this on addressing with the line index.
-   The **Line Offset** describes in what part of the line the actual byte is.
-   A **Hit** occurs when the *requested* tag address matches the *stored* tag address.

::: callout-tip
## Example

1.  First 8 bytes are cached
2.  CPU tries to read M\[1011\]
3.  line: 01, tag: 1
4.  tag in cache dir: 0 $\rightarrow$ **Miss**
5.  cache controller loads line 01, with tag 1: M\[1010, 1011\]
6.  cache directory line 01 is filled with tag: 1

![](images/paste-15.png){fig-align="center" width="8cm"}
:::

::: callout-caution
## Cache Thrashing

-   There is a many to one mapping (address $\rightarrow$ line address)
-   There is only one spot to cache a given memory address

![](images/paste-18.png){fig-align="center" width="7cm" height="3.6cm"}
:::

### Set-associative Caches

-   There are several choices in cache for a given address
-   Equivalent to multiple DM caches in parallel
-   *Reduces* cache thrashing

![](images/paste-17.png){fig-align="center" width="6cm"}

#### 2-Way Set-associative

-   Two DM cahces in parallel
-   Two choices for a given address

![2-way accociative cache example](images/paste-19.png){fig-align="center" width="6cm"}

#### N-Way Set-associative

-   Provides $N$ choices in cache
-   When cache line is loaded, cache needs to choose which of $N$-Lines to use:
    1.  LRU - Least Recently Used

    2.  Random

#### Fully associative cache

-   $N$-Way Set-associative cache for $N$-lines in cache
-   no restrictions on cache line placement $\rightarrow$ **avoids thrashing**
-   $N$ DM caches in parallel with one line each

![Fully associative cache example](images/paste-20.png){fig-align="center" width="6cm"}

## Virtual Memory Systems

If we work with data that can't fit into RAM anymore, we use a *virtual memory system*. The system uses the Harddisk as a higher level cache. The CPU is working with a virtual address, which has to be translated to a physical address.

**Virtual Memory acts like a cache for DRAM in the Hard Disk\
**- cache: 8-64 byte lines\
- VM: 4kB/4MB pages\
- VM: Has horrendous miss penalty (better with SSD)\
- VM creates illusion that CPU has more memory

Advantages of Virtual Memory Systems:

-   Coexistence of multiple programs with separate address spaces with protection from each other
-   Better system security: Accessing invalid pages (e.g. device registers) generates page fault (Segmentation Violation)
-   Simpler program loading (All processes can start at address 0)
-   Faster program startup, since pages are loaded on demand
-   Extension of the address space of a running process
-   Retirement of DRAM pages that have memory faults

### MMU - Memory Management Unit

![](images/paste-34.png){fig-align="center" width="8cm"}

-   MMU translates VA (virtual addr) $\rightarrow$ PA (physical addr)
-   MMU generates a page fault exception if VA is not in DRAM
-   Page fault exception is handled by OS $\rightarrow$ *paging*

### Virtual Addresses / Page Tables

![Page Table (left) and Hierarchical Page Table (right)](images/paste-36.png){fig-align="center" width="9cm"}

Virtual Addresses (VA) are split into a Virtual Page Number (VPN) and a Page Offset. VPN is used to page the Physical Page Number (PPN).

VPN consist of 48 bits and if we'd address 39 bit addresses, we'd need a lookup table of size

$$
\frac{2^{48}\cdot 39\text{ bits}}{8}=1248\text{ TB}
$$

Through the use of pages (4 KB) the size can be reduced to

$$
\frac{2^{48}\cdot39\text{ bits}}{4\cdot 2^{10}\cdot 8}= 312\text{ GB}
$$

or $312$ MB with 4 MB pages.

These pages are still really big, that's why we use **Hierarchical Page Tables**:

-   Page tables and page directories stored in DRAM (can be paged to disk)
-   Reduces page table memory
-   Perform page table walk to get final addr

### TLB - Translation Lookaside Buffer

-   Special cache to avoid cache table walks
-   Some CPUs doe page table walks in HW $\rightarrow$ most in SW
    -   It's not done very often (Amdahl's Rule)
-   Page table walk follows pointers to find PPN from VPN\
    ![](images/paste-35.png)
-   TLB is fully associative (no restrictions where a addr can be stored (penalty is to big if it's not in cache)

### TLB and Caches

Normally TLB look ups are only performed if there is a cache miss on L1 cache.

![](images/paste-37.png)

#### VIVT

**V**irtual **I**ndex **V**irtual **T**ag

-   L1 Cache
-   Translation only occur for cache miss
-   Has aliasing problem: multiple VA $\rightarrow$ PA

#### PIPT

**P**hysical **I**ndex **P**hysical **T**ag

-   L2 Cache

-   Uses translated PPN addresses

#### VIPT

**V**irtual **I**ndex **P**hysical **T**ag

Do the addr translation and cache look up in parallel, that way the PA is ready if there is a cache miss.

![](images/paste-38.png){fig-align="center" width="8cm"}

This is implemented with 3 SRAMs in parallel

![](images/paste-39.png){fig-align="center" width="8cm" height="4.3cm"}

### Virtual Memory Paging (swapping)

![](images/paste-40.png){fig-align="center" width="8cm"}

### VM Page Table Attributes

![](images/paste-41.png){fig-align="center" width="7cm" height="1.7cm"}

-   **Presence** - Set if page resident in DRAM
    -   page fault exception if try to read a page if bit not set
-   **Modified** - Indicates page written (dirty)
    -   needs to be written to disk
-   **Reference** - Set if page accessed
    -   OS uses LRU (least recently used) algorithm to determine if page should be in DRAM

Security improving Attributes:

-   **Read** **Only** - Page fault exception if you try to write to page
-   **Execute** - Indicates page contains instructions
-   **Owner** - Indicates a page that can only be accessed by OS kernel

### IOMMU

MMU for IO devices (e.g. mapping GPU memory to physical addr space)

### Multiple virtual addr spaces

In a multitasking system, each process has its own page table and the CPU selects the current page table using a page table register.

![](images/paste-42.png){fig-align="center" width="6cm"}

## Processor Modes

Different CPU modes with different privileges maintain system security.

![](images/paste-43.png){fig-align="center" width="8cm"}

### Kernel Mode

-   Unrestricted access to hardware, page tables, CPU registers, and privileged instructions

-   Entered when interrupt occurs, the previous processor mode is restored when the ISR executes a return-from-interrupt instruction

### User Mode

-   Used by applications so they cannot directly access hardware devices or memory belonging to other tasks

-   The application must make a system call to access the hardware

## Profiling

To determine the *hotspots* of a program, the code parts which can improve the execution speed the most, we use profiling techniques.

### External Timing

1.  Drive GPIO-Pin **High** at start of function
2.  Drive GPIO-Pin **Low** at end of function
3.  Use *Oscilloscope* to measure the time

### Internal Timing

Use integrated clock functions that approximate the time used by the CPU (`#include <time.h>`).

| **Attention**: Poor resolution and no notice of interrupts
| $\rightarrow$ average over many iterations

### Subroutine Profiler

Used to find which fucntion takes the most time (Tool: **gprof**).

1.  Compile with profiling enabled
    a.  `gcc -pg` (profiling flag)
    b.  Compiler adds extra code to start of every function to count how often it's called
2.  Run program as usual $\rightarrow$ on exit info & additional timing info gets dumped to file
3.  Run profiling analysis (**gprof**) to analyse saved info and annotate the C-code

::: callout-caution
## Flat timing profile

As a result of the analysis, we get a **flat timing profile**, which shows how much time is spent in each function.

If no predominant function (first example) it's harder to determine where to start optimising, where a dominant function ( `func3` second example) gives a better clue.

![](images/paste-21.png){fig-align="center" width="5cm"}

A flat profile comes as a text summary:

![](images/paste-24.png){fig-align="center" width="8cm"}
:::

### Deterministic Profiler

The compuler adds code at the start and end of functions to read the elapsed time.

**Attention**: This adds alot of additional code.

### Statistical Profiler

Periodic interrupt handle that **samples Program Counter** of the interrupted function and stores this in profiler file. This is simple to implement, but subject to sampling artefacts (missing frequent but very short functions).

### Block Profiler

Used to find which line is executed the most $\rightarrow$\$ assume you have to optimise this line.

Tools: **gcov**, **oprofile** (both Linux)

![](images/paste-22.png){fig-align="center" width="8cm"}

::: callout-note
## Basic Block

A group of statements that are always executed together.

No *flow-changing* statements (e.g. `if`, `for`, ...)
:::

#### Code Coverage

With *basic block profiling* we can work out the **code coverage**. We can work out how many times code is executed.

This is useful to detect **dead code**.

![](images/paste-23.png){fig-align="center" width="8cm"}

### Just-in-time Profilers

Just-in-Time profilers can perform the analysis dynamically, generating detailed information, but making the program running slower.

Tools: **callgrind** (from *valgrind*)

### Simulators

A accurate simulator of a CPU can give deep insights about **pipeline stalls**, **cache statistics**, **Clock Cycles per Instruction**, and much more.

They are extremely complex and slow and hard to get for modern (e.g. X86) CPUs.

### Hardware Profilers

Modern CPUs contain registers, that store information like **Clock Cycles per Instruction**, **Wrong Branch Predictions**, **Cache Misses**, ...

Tools: **perf** (Linux, X86)

## Optimisation

Optimisation is mainly done by compilers and optimally produce smaller and faster code. Some speed optimisations do tradeoff with size (e.g. loop unrolling).

-   **Avoid** premature optimisation
-   Use **profiling** tools to find what to optimise first
-   Use a **better algorithm** (e.g. FFT vs. DFT)

### GCC optimisation levels

GCC has built in optimisation (100's of flags), but there are predefined optimisation levels:

-   `-O1` - enable simple optimisations
-   `-O2` - enable `-O1` with additional optimisation, *no speed/memory tradeoff*
-   `-O3` - enable `-O2` with additional optimisation, may make program larger (e.g. loop unrolling)
-   `-Og` - enables optimisation that do not interfere with debugging
-   `-Os` - optimise for size, uses `-O2` optimisations that do not increase code size
-   `-Ofast` - enable `-O3` with additional non standard compliance optimisation

### Aliasing

![](images/paste-25.png){fig-align="center" width="8cm" height="1.9cm"}

**Problem**: If `*pa` and `*pb` point to the same address, the return value should be 0, that's why the compiler has to **play safe**.

**Solution**: Restricted Pointer

![](images/paste-26.png){fig-align="center" width="7cm"}

#### C Aliasing Rules

1.  A pointer to `char` can alias with any other pointer
2.  Pointers to data types that differ only by qualifier can alias. (e.g. `unsigned int` can alias `int`)
3.  Pointers to different built-in types do not alias. (`int` and `double`)
4.  Pointers to aggregate or union types with differing tags do not alias (two separately defined unions)
5.  Pointers to aggregate or union types which differ only by name may alias (two unions based of the same definition)

### Fast and Loose math

FLoating point is a poor aproximation of real numbers, they are special rational numbers of the form

$$
\frac{M}{2^N}
$$

with $M$ and $N$ as integers. Thus numbers such as $0.1$ can't be represented correctly.

![](images/paste-27.png){fig-align="center" width="8cm"}

This operation doesn't give the same result, thus the compiler won't optimise it.

If numeric accuracy is *not important*, this optimisation can be enabled with the `-fast-math` option.

Furthermore, float point arithmetic is non associative, the order matters, because:

![](images/paste-28.png){fig-align="center" width="4cm" height="1.2cm"}

### Assiting the compiler to optimise

**Data Qualifier**:

-   `unsigned` - value never negative
-   `const` - value never change
-   `volatile` - value can unpredictably change (shared ISR variable HW registers) $\rightarrow$ useful to tell the compiler not to optimise

``` c
for (i=0; i<1000; i++){
    continue;
}
// i=1000, would be otpmised away
// but might be used at other point!
```

-   `restrict` - pointer not aliased

**Bounds checking**

![](images/paste-29.png){fig-align="center" width="8cm" height="2.4cm"}

## GPU - Graphics Procesing Unit

Originally designed for graphics, including VRAM and many pipelined FUs *(shaders, texture mapping, render output, special functions \[sine/cosine/...\], ray tracers).*

NVIDIA introduced the **GPGPU** - **G**eneral **P**urpose **GPU**, which got popular for AI. It consists of many more cores than a CPU (1000's).

![](images/paste-31.png){fig-align="center" width="8cm"}

![](images/paste-30.png){fig-align="center" width="8cm"}

### Architecture

![](images/paste-32.png){fig-align="center" width="5cm" height="0.7cm"}

Each Core consists of many functional units (typically 32), but have a lot less cache memory than a CPU.

::: callout-note
## Latency Hiding

Due to the smaller cache size, memory has to be read from main memory more often. To mitigate the introduced latency, **latency hiding** is introduced.

Whenever a thread is waiting for memory, another thread can be run (i.e. multithreading).

This requires less fanc caching, but **fast context switches**. This is achieved by having a set of registers for each thread.
:::

### SIMT - Single Instruction Multiple Threads

-   Group block of threads together, typically 32. \*(NVIDIA: warp, AMD: wavefront)
-   Special warp core (NVIDIA: streaming multiprocessor, AMD: SIMD)
-   Each warp has a shared single instruction pointer
-   Threads in a warp execute in lock-step
-   ! Divergent paths are bad for utilisation of the FUs (branching)

![](images/paste-33.png)

-   Latency hiding: If a warp is blocked, another queued warp is executed.

### GPU Programming

Threads are organised as a grid. It is often **faster to recalculate results, than read them from memory.**

-   CUDA - Compute Unified Device Architecture: NVIDIA framework
-   OpenCL - Open Computing Language: Apple framework, for heterogeneous crossplatform execution (CPUs, GPUs, DSPs, FPGAs)
-   OpenACC - Open Accelerators: Programming standard for parallel computing on heterogeneous CPU/GPU systems

# Advanced Topics and Future Technologies

## Computer exploits

Typical computer exploits are based on reading protected memory through a side channel.

### Side Channel

The Side Channel is the central part of computer exploits and utilises the fact that a cached memory location is faster to read.

1.  Prepare side channel: allocate a probe array and ensure it's not cached *(by reading lots of other memory)*
2.  Fool the CPU to read a protected memory location *(e.g. kernel)* and use the result as an index into the probe array

![](images/paste-44.png){fig-align="center" width="8cm" height="4.3cm"}

### Meltdown

Relies on a flaw with out of order execution.

::: callout-caution
## Page Fault Exception

When we try to access protected memory through `Z=*ptr;` a page fault exception occurs, because access is not allowed.

**Meltdown** works around that with utilising a signal handler.
:::

1.  Set up a signal handler to capture page fault exception otherwise the OS aborts the program
2.  Prepare probe array
3.  Read from protected memory location

``` c
Z = *ptr; // Some CPUs take a long time to check page permissions (pipelining)
y = probe_array(Z*64); // in the meantime this insn is already executing and loading the `Z` part into cache
```

4.  Through timing analysis on accessing the probe array, the value of `Z` can be figured out

Meltdown can be avoided by:

-   Clearing cache on page fault (slow)
-   Disable cache (very slow)
-   Disable out of order execution (slow)
-   Randomize timing \[apple\]
-   Only map a few of the kernel pages in the virtual memory space (e.g. at least the interrupt vectors)

### Spectre

Relies on a flaw with speculative execution. The attacker needs to find a special function that runs with privileges in the kernel or web server. This function must have the form of

``` c
char attack (int x, char *probe_array){
    char y=0;
    if (x<array_size)
        y = probe_array[array[x]*64]; // 64 must be int >= 64
    return y;
}
```

1.  Prepare branch predictor by calling `x<array_size` repeatedly
2.  Ensure probe array is not cached
3.  Call attack function with value of x outside of array bounds
4.  Use memory timing to infer secret value

![](images/paste-45.png){fig-align="center" width="3cm" height="6.6cm"}

Spectre can be avoided by:

-   Disable cache (really slow)
-   Disable speculative execution (slow)
-   Randomize timing
-   Use compiler to avoid generating code that can be exploited

## Instruction set architecture problems

## The ARM Cortex A-15

## Quantum computing

## Quantum computers (superposition)

## Quantum computers (entanglement)